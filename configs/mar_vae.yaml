autoencoder:
  binary_dim: &binary_dim 16
  vae_dim: &vae_dim 16
  num_latents: &num_latents 256
  encoder:
    vae_dim: *vae_dim
    embed_dim: 768
    num_layers: 6
    num_latents: *num_latents
    num_heads: 32
    output_dim: *binary_dim

  decoder:
    vae_dim: *vae_dim
    embed_dim: 768
    num_layers: 6
    num_latents: *num_latents
    num_heads: 32
    input_dim: *binary_dim
    recon_levels: [1, 2, 3, 4, 5, 6, 8, 10, 13, 16] # the same as VAR

train:
  gpt_resume_path:
  global_step: 1
  exp_name: &exp_name 'distill_MAR_VAE'
  wandb_proj: *exp_name
  output_dir: *exp_name
  logging_dir: 'logs'
  mixed_precision: 'bf16'
  gradient_accumulation_steps: 2
  report_to: 'wandb'
  num_iters: 10000
  save_every: 2000

data:
  path: '/data/Largedata/ImageNet_wds/imagenet-train-{000000..001281}.tar'
  batch_size: 45 # to modify