# 套娃512, latent的bits逐渐递增
autoencoder:
  binary_dim: &binary_dim 16
  vae_dim: &vae_dim 16
  num_latents: &num_latents 512
  matryoshka: true
  encoder:
    vae_dim: *vae_dim
    embed_dim: 512
    num_layers: 8
    num_latents: *num_latents
    num_heads: 8
    output_dim: *binary_dim
  
  decoder_matryoshka:
    binary_mode: true
    vae_dim: *vae_dim
    embed_dim: 512
    num_layers: 8
    num_latents: *num_latents
    num_heads: 8
    input_dim: *binary_dim
    latents_mask_schedule: 'linear'

train:
  resume_path: 'experiment/matryoshka_new_attn_mask/AE-distill_MAR_VAE-10k'
  # skipped_keys:
  skipped_keys: ['encoder.latents', 'decoder.latents_pos_embed']
  global_step:
  exp_name: &exp_name 'distill_MAR_VAE'
  wandb_proj: *exp_name
  # output_dir: *exp_name
  output_dir: 'matryoshka_casual_latents_incre'
  logging_dir: 'logs'
  mixed_precision: 'bf16'
  gradient_accumulation_steps: 1
  report_to: 'wandb'
  num_iters: 20000
  # val_every: 10
  save_every: 5000

data:
  path: '/data/Largedata/ImageNet_wds/imagenet-train-{000000..001281}.tar'
  batch_size: 60 # to modify