autoencoder:
  # vae_dim: &vae_dim 16
  vae_dim: &vae_dim 256
  num_latents: &num_latents 512
  binary_dim: &binary_dim 64
  outer_ckpt:
  inner_ckpt:

  encoder_1d:
    vae_dim: *vae_dim
    embed_dim: 512
    num_layers: 8
    num_latents: *num_latents
    num_heads: 8
    output_dim: *binary_dim
  decoder_1d:
    binary_mode: true
    vae_dim: *vae_dim
    embed_dim: 512
    num_layers: 8
    num_latents: *num_latents
    num_heads: 8
    input_dim: *binary_dim
    latents_mask_schedule: 'linear_64'

hybrid_loss:
  disc_start: 1
  # disc_weight: 0.05
  disc_weight: 0.03

train:
  resume_path: '/data/experiment/ae_total/linear_64_z=256/AE-ae_total-3k'
  skipped_keys: 
  # skipped_keys: ['encoder.conv_out.weight', 'encoder.conv_out.bias', 'decoder.conv_in.weight', 'decoder.conv_in.bias', 'quant_conv.weight', 'quant_conv.bias', 'post_quant_conv.weight', 'post_quant_conv.bias', 'encoder_1d.patch_embed.weight', 'decoder_1d.output.weight', 'decoder_1d.output.bias']
  loss_resume_path: '/data/experiment/ae_total/linear_64_z=256/Loss-ae_total-3k'
  # resume_path: 'experiment/ae_total/512_24/AE-ae_total-10k'
  # skipped_keys: ['encoder_1d.output.weight', 'encoder_1d.output.bias', 'encoder_1d.output.weight', encoder_1d.output.bias', 'decoder_1d.latents_mask', 'decoder_1d.input.weight', 'decoder_1d.input.bias']
  global_step: 3001
  exp_name: &exp_name 'ae_total'
  wandb_proj: *exp_name
  # output_dir: *exp_name
  output_dir: 'ae_total/linear_64_z=256'
  logging_dir: 'logs'
  mixed_precision: 'bf16'
  gradient_accumulation_steps: 2
  report_to: 'wandb'
  num_iters: 100000
  # val_every: 50
  save_every: 1000
  # hp_matryoshka: 0.1
  hp_matryoshka: 0.05

data:
  path: '/data/imagenet/imagenet-train-{000000..001281}.tar'
  batch_size: 20